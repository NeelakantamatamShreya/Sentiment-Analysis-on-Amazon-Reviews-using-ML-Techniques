
# ğŸ§  Sentiment Analysis on Amazon Reviews using Machine Learning

A machine learning project that classifies Amazon product reviews into **positive**, **negative**, or **neutral** sentiments using classical ML algorithms and feature extraction techniques.

---

## ğŸ“– Abstract

In todayâ€™s digital era, customer feedback is a critical source of business intelligence. This project aims to **automate sentiment analysis** of Amazon product reviews using **machine learning techniques**.
After experimenting with multiple classifiers and vectorization methods, the **Nu-SVM** model achieved the **highest accuracy of 94%**, proving to be the most effective for sentiment prediction.
This analysis provides valuable insights into customer opinions, helping e-commerce platforms and sellers make **data-driven decisions**.

---

## ğŸš€ Features

* âœ… Data preprocessing and cleaning of raw Amazon review data
* ğŸ§© Feature extraction using **CountVectorizer** and **TF-IDF Vectorizer**
* âš™ï¸ Model training using multiple ML algorithms:

  * Naive Bayes
  * Logistic Regression
  * Random Forest
  * Support Vector Machine (SVM)
* ğŸ¯ Hyperparameter tuning using **RandomizedSearchCV** and **Nu-SVM**
* ğŸ“ˆ Model evaluation using accuracy, precision, recall, and F1-score
* ğŸ“Š Visualization with confusion matrices and sentiment distribution charts

---

## ğŸ› ï¸ Tech Stack

* **Programming Language:** Python
* **Libraries:** scikit-learn, pandas, NumPy, Matplotlib, Seaborn
* **Environment:** Jupyter Notebook / Google Colab

---

## ğŸ“‚ Dataset

The dataset was sourced from **[Kaggle](https://www.kaggle.com/)** under the search term *â€œAmazon Reviews.â€*
Due to licensing restrictions, the dataset is **not included** in this repository.

After downloading, place it inside the `data/` folder as:

```
data/amazon_reviews.csv
```

---

## ğŸ§© Methodology

1. **Data Collection:** Downloaded the Amazon reviews dataset from Kaggle.
2. **Preprocessing:**

   * Handled missing values.
   * Combined `review_title` and `review_text` into one column.
   * Labeled sentiments based on ratings:

     * â­ 4â€“5 â†’ Positive
     * â­ 3 â†’ Neutral
     * â­ 1â€“2 â†’ Negative
3. **Feature Extraction:** Applied CountVectorizer and TF-IDF to convert text into numerical form.
4. **Model Training:** Trained multiple classifiers â€” Naive Bayes, Logistic Regression, Random Forest, and SVM.
5. **Evaluation:** Compared all models using accuracy, precision, recall, and F1-score.
6. **Fine-Tuning:** Optimized Random Forest using RandomizedSearchCV and SVM with Nu-SVM.
7. **Visualization:** Plotted sentiment distribution and confusion matrices for analysis.

---

## ğŸ“Š Results & Model Comparison

### ğŸ”¹ CountVectorizer Results

| Algorithm           | Accuracy | Best Precision  | Best Recall    | Best F1-Score   |
| ------------------- | -------- | --------------- | -------------- | --------------- |
| Naive Bayes         | 92%      | Positive (0.92) | Positive (1.0) | Positive (0.96) |
| Logistic Regression | 90%      | Neutral (1.0)   | Positive (1.0) | Positive (0.94) |
| **Random Forest**   | **93%**  | Neutral (1.0)   | Positive (1.0) | Positive (0.96) |
| **SVM**             | **93%**  | Negative (0.96) | Positive (1.0) | Positive (0.96) |

> âœ… **Observation:** Both **Random Forest** and **SVM** achieved the **highest accuracy of 93%** using the **CountVectorizer** approach, outperforming Naive Bayes and Logistic Regression.

---

### ğŸ”¹ TF-IDF Vectorizer Results

| Algorithm           | Accuracy | Best Precision  | Best Recall     | Best F1-Score   |
| ------------------- | -------- | --------------- | --------------- | --------------- |
| Naive Bayes         | 91%      | Positive (0.93) | Positive (0.98) | Positive (0.95) |
| Logistic Regression | 92%      | Positive (0.93) | Positive (0.99) | Positive (0.96) |
| **Random Forest**   | **93%**  | Neutral (1.0)   | Positive (1.0)  | Positive (0.96) |
| **SVM**             | **93%**  | Positive (0.95) | Positive (0.98) | Positive (0.97) |

> âœ… **Observation:** Similarly, both **Random Forest** and **SVM** maintained the **same top accuracy (93%)** with the **TF-IDF Vectorizer**, showing consistent performance across feature extraction methods.

---

### ğŸ”¹ Fine-Tuned Models

| Algorithm             | Accuracy | Precision (Positive) | Recall (Positive) | F1-Score (Positive) |
| --------------------- | -------- | -------------------- | ----------------- | ------------------- |
| Random Forest (Tuned) | 90%      | 0.90                 | 1.0               | 0.95                |
| ğŸ† **Nu-SVM (Best)**  | **94%**  | **0.94**             | **0.99**          | **0.97**            |

> ğŸ§  **Conclusion:** Both **Random Forest** and **SVM** performed equally well with 93% accuracy on both vectorizers, but after fine-tuning, **Nu-SVM** achieved the **highest accuracy of 94%**, making it the most effective model overall.

---

## ğŸ“š Publication

This project was **accepted for publication** at the
ğŸ“ *3rd World Conference on Information Systems for Business Management (ISBM)* â€” **Springer LNNS Series (2024)**.

ğŸ“– [Springer Publication Link â€” Coming Soon]()

---

## ğŸ“Œ Conclusion

* Both **Random Forest** and **SVM** achieved **93% accuracy** across both CountVectorizer and TF-IDF vectorization methods.
* Fine-tuning with **Nu-SVM** improved performance to **94%**, confirming it as the most efficient model.
* This project demonstrates how **vectorization strategies** and **hyperparameter tuning** significantly influence model performance in sentiment analysis.

---

## ğŸ”® Future Scope

* ğŸ¤– Integrate **deep learning models** like **LSTM** and **BERT** for contextual understanding.
* ğŸŒ Expand the system for **multilingual** sentiment classification.
* ğŸ§­ Develop an **aspect-based sentiment analyzer** for more granular insights.
* ğŸ’» Build a **web interface or dashboard** for real-time sentiment tracking and visualization.

---

## ğŸ™Œ Contributors

ğŸ‘©â€ğŸ’» **Neelakantamatam Shreya**
ğŸ‘©â€ğŸ’» **Indukuri Varsha**
ğŸ‘©â€ğŸ’» **Gottumukkala Kavya**

---

## ğŸ“¬ Contact

ğŸ“§ **Email:** [shreyanm6@gmail.com](mailto:shreyanm6@gmail.com)
ğŸ”— **LinkedIn:** [linkedin.com/in/neelakantamatam-shreya](https://linkedin.com/in/neelakantamatam-shreya)

---

### â­ Donâ€™t forget to star this repository if you found it helpful!

---

Would you like me to make it even more **visually appealing** for GitHub (with shields/badges for Python, scikit-learn, license, etc.)? That helps your profile stand out even more to recruiters.


